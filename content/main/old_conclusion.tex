% !TEX root = ../../thesis.tex

\Section{Conclusion}

\glsresetall{}

This thesis presented three different methods that involved fitting between two high-dimensional data distributions using \gls{dl}. However, the resulting implementations differed because of the different physical goals.

The data generation using \glspl{wgan} and the refining of data through deep \glspl{sf} is based on fitting data distributions without exactly copying from one another. The adversarial approach was very successful in condensing and generalizing the information of the differences between two datasets into a separate \gls{dnn}. It is important to select the correct training loss for such an adversary. Depending on the task, the adversary should be trained in such a way, that the result can be used in an efficient way by the main network. This has been the critic in the case of gradients along the input variables and a classical discriminator in the case of meaningful gradients for the weights. In contrast to these two methods, the fit of \glspl{cr} was targeted to explicitly reproduce the exact data, thus an adversarial approach was discarded and a simple regression loss used instead.

The parameters of the models had also very different tasks. For the \gls{wgan} generation the parameters of the generator network had to transform a high-dimensional normal-distributed input into a highly correlated set of variables, potentially encoding physics in these parameters that describe the dependencies between the output features. However, the task of training this network seemed to be particularly hard as the different variables had very different structures and a lot of noise. In comparison, \gls{gan}-like approaches are extremely successful for image-like data with the two main reasons being the exploitation of the highly efficient \gls{cnn} structure and images having a very sparse information density compared to their full dimensionality. The results presented in this thesis should still give rise to concerns as it is not very common for publications to quantify and validate in a truly general way, how successful (W)GANs capture correlations in the training data. % chktex 36

The parameters in the SF-Net just had to produce a single output which was much more deterministic with respect to the inputs. This approach performed therefore much better. The functional representation could be used to gain insights on how the \glspl{sf} depend on local variations of the input via a gradient analysis. This method not only has much more potential for the presented task inside \gls{cms} but can possibly be transferred to many more refinement tasks of simulated data, as this is a good middle ground between \gls{dl} based data refinement and methods with controllable uncertainties.

The fit of \glspl{cr} had the hidden physics variables encoded as the parameters of the model. These parameters could directly be used for further analyses, e.g.\ comparing reconstructed directions with other astronomical observations and investigating the fitted charge composition. In general, the \gls{dl} techniques were used to produce a differentiable transformation based on a given physics model. The efficient method of gradient descent was then applied to essentially find the input of the physics model that created the measured output of the transformation. The loss can, in principle, be easily used to focus on different objectives of the fit and simple adjustments can be tested fast to improve the results. This method worked in the presented case very well and the general concept seems to have many more possible applications.

Overall, this thesis has shown that \glspl{dnn} are a very appropriate tool for comparing and fitting multi-dimensional data. As \gls{dl} offers a wide variety of techniques, a lot of tasks in this area can probably be compared to an equivalent \gls{dl} setup. With the loss functions of these models being the most important aspect of the training, it is easy and relatively simple to adjust these for different purposes leading to a fast development cycle.
