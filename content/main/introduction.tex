% !TEX root = ../../thesis.tex

\Section{Introduction}\label{sec:introduction}

Modern particle physics analyses make extensive use of computing and data storing resources. Larger amounts of data reduce statistical fluctuations and make the discovery of rare physics possible. Large analysis workflows and detailed physics simulations improve the search for new physics, the precision-measurement of physical constants or simply enable physicists to evaluate datasets that have only scientific value after aggregation and reconstruction.

A more recent development is the application of Deep Learning methods within such analyses. In particular, they had a big success in replacing classical discrimination tasks based on a large number of input variables, as the development within a relatively short amount of time is able to outperform traditional algorithms. The main reason for their advance is the fact that they can exploit local correlations in high-dimensional data much faster and more accurate than humans do through hand-crafted algorithms~\cite{intro-dnn1, intro-dnn2}.

Analysis workflows do not only consist of discrimination tasks but many others that operate on high-dimensional data as well. A promising subfield of such tasks seems to be the fitting and comparing of large datasets for different purposes.

In this thesis, three different tasks concerning the fitting of high-dimensional data in the field of particle physics are presented. The first two are concerning the production of simulated data in the setting of a High Energy Physics experiment, the CMS detector~\cite{cms}. The first method will investigate the possibility of fitting an output distribution of a neural network to a given data distribution of simulated events, resulting in a possible increased production efficiency of simulations while retaining the necessary structure of the data. The second study focuses on a post-processing step of simulated events, called reweighting. An alternative approach using deep neural networks is proposed, to create event weights that better fit the simulated data distribution to the measured data of the experiment. The third method is in the context of Astroparticle Physics and the \pao{}~\cite{ThePierreAugerCollaboration2016} and aims at reproducing the measured data of thousands of cosmic rays through a simulated propagation of parameterized origins and charges, that are fitted in a novel way using Deep Learning techniques.

While all three tasks are situated around the general task of fitting high-dimensional data, their goals in terms of physical results are inherently different. The first task deals with a computational efficient simulation of data. The second task takes an already simulated set and tries to refine it to measured data to avoid transfer learning problems between data and simulation. The third task is about inverting a functional dependence of the measured data to retrieve the hidden physics information by performing a high-dimensional fit.

Before going into the details of the different methods the thesis will start with the presentation of the two particle physics experiments that provide the physical background. Afterward, a section on Deep Learning introduces the general theory and techniques that are used throughout this thesis. Then each method will be demonstrated while pointing out the differences and similarities to the other methods in the more general context of data analyses. We will understand what aspects of the methods work well and why some results do not live up to their expectations. The conclusion will point out the most promising methods for future analyses based on the results of this thesis.
\todo{intro is spellchecked}
