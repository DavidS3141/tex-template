% !TEX root = ../../thesis.tex

\Section{Conclusion}

\glsresetall{}

This thesis presented three different methods that involved fitting between two multi-dimensional data distributions using modern \gls{dl} techniques in the context of particle physics applications.

Traditionally, multi-dimensional data is evaluated and compared by extracting important features using expert-knowledge of the data. In contrast, the presented methods used gradient descent through backpropagation and \glspl{dnn}. This thesis has shown that these tools are much more appropriate as they provide a general way of multi-dimensional fitting which is not biased against some human-engineered features. The gradient descent allows for an efficient navigation in large multi-dimensional space (cf. \refsec{magnet}) while the \glspl{dnn} provide the ability to encode discrete data distributions in a continuous manner thus not suffering from low phase space densities caused by the many dimensions (cf. \refsec{mcwgan},~\ref{sec:deepsf}). One of the most important aspects of each method was the used loss function which guides the fit to the desired solution.

The first method, which trained a generator \gls{dnn} to simulate Monte Carlo events, introduced the Wasserstein distance and how it can be used inside the loss function. As a well-defined metric between two data distributions, it was the main reason for an increase in performance and training stability compared to more common objectives.

Instead of generating data, the second method's goal was to produce sample weights for data events that would refine an already existing simulated dataset to match more closely a measured dataset. A novel and custom-designed loss function was presented which was very successful in producing the weights using a \gls{dnn} output. The differentiable mapping of input events to output weights allowed the inspection of the functional dependence of the weights with respect to the different input variables. This has been impossible with the traditional method. Additionally, this method can safely be extended to more input dimensions making it an interesting alternative for future applications.

An important foundation for both of these methods was the so-called adversarial concept. This concept describes the usage of a second network which is trained simultaneously with the target \gls{dnn}. The second network is called an adversary, as its output is used in the loss function of the target network. The usage of an adversary in both methods made it possible to use the discriminating power of a \gls{dnn} as a training target guiding the target network towards a favorable solution.

The third method presented a novel fit in the context of reconstructing extragalactic source directions. The fit features a purpose-built loss function which directly encodes the underlying scientific question of anisotropy in the source directions. Additionally, the objective made also use of a non-differentiable physical model of the galactic magnetic field. As the objective needs to be differentiable, a simple yet very successful approach was carried out. The model of the field was encoded into a \gls{dnn} through an ordinary \gls{dl} training, not only making this fit possible but also showing overall promising results.

The usage of \gls{dl} and more importantly, the construction of the loss function was an essential ingredient in all three cases. The development of the objective was done in a mathematically motivated top-down approach, meaning that the formulated goals instead of the technical possibilities were used as starting points. This leads to a more justified and creative use of the available tools and worked well for constructing task-specific objectives.

Overall, this thesis has shown that \glspl{dnn} are a very appropriate tool for comparing and fitting multi-dimensional data. As \gls{dl} offers a wide variety of techniques, a lot of tasks in the domain of multi-dimensional fits can be solved using similar strategies to the ones presented here. The usage of \glspl{dnn} allows the methods to use more raw data and less human-engineered features resulting in an optimized extraction of the contained information. The most essential part of most \gls{dl} setups is the loss function. With the many possibilities of constructing such an objective term within only a few lines of code, it is relatively simple to adjust it for different purposes leading to a fast development cycle.
